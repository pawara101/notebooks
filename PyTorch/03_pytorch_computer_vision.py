# -*- coding: utf-8 -*-
"""03_pytorch_computer_vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1daNsoOs3-0eDxjKLipyH24QZVFUCuEwi

# PyTorch Computer vision

---
## 0.Computer vision libs in PyTorch.
* use **Torchvision**
* `torchvision.datasets` - Get datasets and data loading functions
* `torchvision.models` - get pre-trained computer vision models
* `torchvision.transforms` - functions for manipulation your vision data to be suitable for use with an ML model.
* `torch.utils.Dataset` - Base dataset class for PyTorch
* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset
"""

import torch
from torch import nn

## Import TorchVision
import torchvision
from torchvision import datasets
from torchvision import transforms
from torchvision.transforms import ToTensor

import matplotlib.pyplot as plt

## Check Versions
print(torch.__version__)
print(torchvision.__version__)

"""## 1.Getting a dataset
* Using FashionMNIST Data-set
"""

## Setup Trainning Data
from torchvision import datasets

train_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=torchvision.transforms.ToTensor(),
    target_transform=None
)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=torchvision.transforms.ToTensor(),
    target_transform=None
)

len(train_data),len(test_data)

[image,label]=train_data[0]

class_names = train_data.classes
class_names

class_to_idx = train_data.class_to_idx
class_to_idx

train_data.targets

## Check the shapes
print(f"Image Shape : {image.shape}")
print(f"class label : {class_names[label]}")

"""### 1.2 Visualising our Data"""

image,label  = train_data[0]
print(f"Image Shape : {image.shape}")

plt.imshow(image.squeeze()) ## removes singular dimention

plt.imshow(image.squeeze(),cmap="gray")
plt.title(class_names[label])

## Plot more images
torch.manual_seed(42)
fig = plt.figure(figsize=(6,6))
rows,cols = 4,4
for i in range(1,rows*cols+1):
  random_idx = torch.randint(0,len(train_data),size=[1]).item()
  # print(random_idx)
  img, label = train_data[random_idx]
  fig.add_subplot(rows,cols,i)
  plt.imshow(img.squeeze(), cmap="gray")
  plt.title(class_names[label])
  plt.axis(False)

"""## 2. Prepare DataLoader
* DataLoader turns our dataset into a python iterable.
* We want to specifically we want turn our data inot mini batches.
    - why we are doing this ?
      - more efficient
      - gives neural network more chances to update its gradient per epoch.
"""

from torch.utils.data import DataLoader

## setup batch size
BATCH_SIZE = 32

## Turn datasets into iterables
train_dataloader = DataLoader(dataset=train_data,
                              batch_size=BATCH_SIZE,
                              shuffle=True)

test_dataloader = DataLoader(dataset=test_data,
                             batch_size=BATCH_SIZE,
                             shuffle=False)

print(f"Data Loaders : {train_dataloader,test_dataloader}")
print(f"Length of the train_dataloader: {len(train_dataloader)} Batches of {BATCH_SIZE}....")
print(f"Length of the test_dataloader: {len(test_dataloader)} Batches of {BATCH_SIZE}....")

##t
train_features_batch,train_labels_batch = next(iter(train_dataloader))
train_features_batch.shape,train_labels_batch.shape

## Show a Sample
torch.manual_seed(42)
random_idx = torch.randint(0,len(train_features_batch),size=[1]).item()
img,label = train_features_batch[random_idx],train_labels_batch[random_idx]
plt.imshow(img.squeeze(),cmap="gray")
plt.title(class_names[label])
plt.axis(False)
print(f"Image Size : {img.shape}")
print(f"Label : {label} ,Label Size : {label.shape}")

"""## 3. Model 0 : Build a BaseLine Model
* When starting to build series of machine learning modelling experiments,as a practice start with a baseline model.

"""

## Create a flatten layer.
flatten_model = nn.Flatten()

# get a single sample
x = train_features_batch[0]

# Flatten the sample
output = flatten_model(x)  # perform forward pass

# print out
print(f"Shape before flattening -> {x.shape}")
print(f"Shape afer flattening -> {output.shape}")

print(output)

from torch import nn

class FashionMNISTModelV0(nn.Module):
  def __init__(self,
               input_shape: int,
               hidden_units: int,
               output_shape: int):
    super().__init__()

    self.layer_stack = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=input_shape,out_features=hidden_units),
        nn.Linear(in_features=hidden_units,out_features=output_shape)
    )


  def forward(self,x):
    return self.layer_stack(x)

torch.manual_seed(42)

model_0 = FashionMNISTModelV0(
    input_shape=784,
    output_shape=len(class_names),
    hidden_units=10
).to("cpu")

model_0

dummy_x = torch.rand([1,1,28,28])
model_0(dummy_x)

model_0.state_dict()

"""### 3.1 Setup Loss,Optimizer & Evaluation metrics
* Loss function - working with multiclass data using `nn.CrossEntropyLoss()`
* Optimizer - our optimizer will be `nn.torch.SGD()`
* Evaluation Metric.
"""

import requests
from pathlib import Path

## Download helper function
if Path("helper_functions.py").is_file():
  print("helper_functions.py exists")

else:
  print("Downloading helper_functions.py")

  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py","wb") as f:
    f.write(request.content)

from helper_functions import accuracy_fn

# setup loss function and optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.1)

"""### 3.2 Create a function to time the experiment
Need to track :
1. Model's performance.
2. How fast it runs.

"""

from timeit import default_timer as timer

def print_train_time(start:float,
                     end:float,
                     device: torch.device=None):
  total_time = end - start

  print(f"Train Time on {device}: {total_time:.3f} seconds")
  return total_time

start_time = timer()

end_time = timer()

print_train_time(start=start_time,end=end_time,device="cpu")

"""### 3.3 Creating a Trainning Loop and Trainnig model
1. Loop through epochs.
2. Loop through trainning batches, perform trainning steps, calculate the train loss.
3. Loop through test batches, perform trainning steps, calculate the train loss.
4. print whats happening.
5. count the time
"""

from tqdm.auto import tqdm

## set the seed
torch.manual_seed(42)
train_time_start_on_cpu = timer()

## Set number of epochs
epochs = 3

## Create
for epoch in tqdm(range(epochs)):
  print(f"Epoch : {epoch}\n----------")

  ## Trainning
  train_loss = 0

  ## Add a loop to loop thorugh the trainning batches.
  for batch ,(X,y) in enumerate(train_dataloader):
    model_0.train()

    # 1.Forward pass
    y_pred = model_0(X)

    # 2.Calculate the loss
    loss = loss_fn(y_pred,y)
    train_loss +=loss

    # 3.Optimizer
    optimizer.zero_grad()

    # 4.Loss backward
    loss.backward()

    # 5. Optimizer step
    optimizer.step()

    ## Print

    if batch % 400 == 0:
      print(f"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples")

  # Divide total train loss
  train_loss /= len(train_dataloader)

  ## Testing
  test_loss, test_acc = 0, 0
  model_0.eval()

  with torch.inference_mode():
    for X_test,y_test in test_dataloader:
      # 1. forward pass
      test_pred = model_0(X_test)

      # 2. Calculate the loss
      test_loss += loss_fn(test_pred, y_test)

      #. 3. Calculate accuracy
      test_acc += accuracy_fn(y_true=y_test,y_pred=test_pred.argmax(dim=1))


    # Calculate the test loss average per batch
    test_loss /= len(test_dataloader)

    # Calculate the test accuracy
    test_acc /= len(test_dataloader)



  # print
  print(f"\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\n")

# Calculate training time
train_time_end_on_cpu = timer()
total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,
                                           end=train_time_end_on_cpu,
                                           device=str(next(model_0.parameters()).device))

"""## 4. Make predictions and get model_0 results"""

torch.manual_seed(42)
def eval_model(model: torch.nn.Module,
              data_loader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              accuracy_fn):

  loss,acc =0,0

  model.eval()


  with torch.inference_mode():
    for X,y in tqdm(data_loader):

      y_pred = model(X)

      loss += loss_fn(y_pred,y)
      acc += accuracy_fn(y_true=y,
                         y_pred=y_pred.argmax(dim=1))

      ##
      loss /= len(data_loader)
      acc /= len(data_loader)



  return {"model_name" : model.__class__.__name__,
          "model_loss" : loss.item(),
          "model_acc" : acc}


## Calculate model0 results
model_0_results = eval_model(model=model_0,
                            data_loader=test_dataloader,
                            loss_fn=loss_fn,
                            accuracy_fn=accuracy_fn)


model_0_results

"""## 5. Setup device agnostic-code (for GPU using is there is one)"""

torch.cuda.is_available()

# Setup device agnostic code
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""## 6. Model 1: Building a better model with non-linearity"""

# create a model with non linear & linear layers
class FashionMNISTModelV1(nn.Module):
  def __init__(self,input_shape: int,hidden_units: int,output_shape: int):
    super().__init__()

    self.layer_stack = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=input_shape,
                  out_features=hidden_units),
        nn.ReLU(),
        nn.Linear(in_features=hidden_units,
                  out_features=output_shape),
        nn.ReLU()
    )

  def forward(self, x:torch.Tensor):
    return self.layer_stack(x)

## Create an instance of model_1
torch.manual_seed(42)

model_1 = FashionMNISTModelV1(input_shape=784,hidden_units=10,output_shape=len(class_names))
next(model_1.parameters())

"""### 6.1 Setup Loss,optimizer and evaluation metrics"""

from helper_functions import accuracy_fn
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr=0.1)

"""### 6.2 Functionizing trainning and evaluation /testing loops
let's create a function for:
* training loop - `train_step()`
* testing loop - `test_loop()`
"""

def train_step(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               accuracy_fn,
               device: torch.device = device):
    train_loss, train_acc = 0, 0
    model.to(device)
    for batch, (X, y) in enumerate(data_loader):
        # Send data to GPU
        X, y = X.to(device), y.to(device)

        # 1. Forward pass
        y_pred = model(X)

        # 2. Calculate loss
        loss = loss_fn(y_pred, y)
        train_loss += loss
        train_acc += accuracy_fn(y_true=y,
                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels

        # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()

    # Calculate loss and accuracy per epoch and print out what's happening
    train_loss /= len(data_loader)
    train_acc /= len(data_loader)
    print(f"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%")

def test_step(data_loader: torch.utils.data.DataLoader,
              model: torch.nn.Module,
              loss_fn: torch.nn.Module,
              accuracy_fn,
              device: torch.device = device):
    test_loss, test_acc = 0, 0
    model.to(device)
    model.eval() # put model in eval mode
    # Turn on inference context manager
    with torch.inference_mode():
        for X, y in data_loader:
            # Send data to GPU
            X, y = X.to(device), y.to(device)

            # 1. Forward pass
            test_pred = model(X)

            # 2. Calculate loss and accuracy
            test_loss += loss_fn(test_pred, y)
            test_acc += accuracy_fn(y_true=y,
                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels
            )

        # Adjust metrics and print out
        test_loss /= len(data_loader)
        test_acc /= len(data_loader)
        print(f"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\n")

torch.manual_seed(42)

# Measure time
from timeit import default_timer as timer
train_time_start_on_gpu = timer()

epochs = 3
for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch}\n---------")
    train_step(data_loader=train_dataloader,
        model=model_1,
        loss_fn=loss_fn,
        optimizer=optimizer,
        accuracy_fn=accuracy_fn
    )
    test_step(data_loader=test_dataloader,
        model=model_1,
        loss_fn=loss_fn,
        accuracy_fn=accuracy_fn
    )

train_time_end_on_gpu = timer()
total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,
                                            end=train_time_end_on_gpu,
                                            device=device)

"""> **Note :** sometimes depending on hardware you fine cpu is faster than gpu.
>
>1. hardware in CPU has better than GPU.
>

"""

model_0_results

## Calculate model0 results
model_1_results = eval_model(model=model_1,
                            data_loader=test_dataloader,
                            loss_fn=loss_fn,
                            accuracy_fn=accuracy_fn)


model_1_results

from logging import logProcesses
torch.manual_seed(42)
def eval_model(model: torch.nn.Module,
              data_loader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              accuracy_fn,
               device=device):

  loss,acc =0,0

  model.eval()


  with torch.inference_mode():
    for X,y in tqdm(data_loader):
      X = X.to(device)
      y = y.to(device)
      y_pred = model(X)

      loss += loss_fn(y_pred,y)
      acc += accuracy_fn(y_true=y,
                         y_pred=y_pred.argmax(dim=1))

      ##
      loss /= len(data_loader)
      acc /= len(data_loader)



  return {"model_name" : model.__class__.__name__,
          "model_loss" : loss.item(),
          "model_acc" : acc}

model_1_results = eval_model(model=model_1,
                             data_loader=test_dataloader,
                             loss_fn=loss_fn,
                             accuracy_fn=accuracy_fn,
                             device=device)

model_1_results

"""## Model 2: Building a Convolutional Neural Network(CNN)

check [CNN Explainer](https://poloclub.github.io/cnn-explainer/)
"""

## Create a Convolutional neural Network.
## Tiny VGG network
class FashionMNISTModelV2(nn.Module):

  def __init__(self,input_shape:int,hidden_units:int,output_shape:int):
    super().__init__()
    self.conv_block_1 = nn.Sequential(
        ## Conv layer
        nn.Conv2d(in_channels=input_shape,
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size= 3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2)
    )

    self.conv_block_2 = nn.Sequential(
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  padding=1),
        nn.ReLU(),
        nn.Conv2d(in_channels=hidden_units,
                  out_channels=hidden_units,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2)
    )

    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features=hidden_units*7*7,
                  out_features=output_shape)
    )

  def forward(self,x):
    x = self.conv_block_1(x)
    #print(f"Output shape of conv_block_1 {x.shape}")
    x = self.conv_block_2(x)
    #print(f"Output shape of conv_block_2 {x.shape}")
    x = self.classifier(x)

    return x

torch.manual_seed(42)
model_2 = FashionMNISTModelV2(input_shape=1, ## colour channels
                              hidden_units=10,
                              output_shape=len(class_names)).to(device)

model_2

plt.imshow(image.squeeze(),cmap='gray')

rand_image_tensor = torch.randn(size=(1,28,28))
rand_image_tensor.shape

model_2(rand_image_tensor.unsqueeze(0))



model_2.state_dict()

"""### 7.1 Stepping through `nn.Conv2d()`
see the documentation for more information.
"""

torch.manual_seed(42)

# Create batch of images
images = torch.randn(size=(32,3,64,64))
test_image = images[0]

print(f"Image Batch Shape : {images.shape}")
print(f"Single Image Shape : {test_image.shape}")
print(f"Test Image:\n {test_image}")

torch.manual_seed(42)
## Create a single conv2d layer
conv_layer = nn.Conv2d(in_channels=3,
                       out_channels=10,
                       kernel_size=3,
                       stride=1,
                       padding=1)

# pass the data through convulution layer
conv_output = conv_layer(test_image)
conv_output

conv_output.shape

"""### 7.2 Stepping through `nn.MaxPool2d()`"""

test_image.shape

## Print out original image shape
print(f"Test Image original shape :{test_image.shape}")
print(f"Test image with unsqueezed dimensions :{test_image.unsqueeze(0).shape}")

## Create a sample nn.MaxPool2d layer
max_pool_layer = nn.MaxPool2d(kernel_size=2)

## Pass through the conv layer
test_image_thorugh_conv = conv_layer(test_image.unsqueeze(dim=0))
print(f"Shape after gping through conv_layer() :{test_image_thorugh_conv.shape}")

## Pass thorugh the max pool layer
test_image_thorugh_conv_and_max_pool =  max_pool_layer(test_image_thorugh_conv)
print(f"Shape after gping through conv_layer() and max_pool_layer() : {test_image_thorugh_conv_and_max_pool.shape}")

torch.manual_seed(42)
## Create a Random tensor
random_tensor = torch.randn(size=(1,1,2,2))
print(f"\nRandom tensor :\n {random_tensor}")
print(f"Max pool tensor shape: {random_tensor.shape}")
# Create a maxpool layer
max_pool_layer = nn.MaxPool2d(kernel_size=2)

max_pool_tensor = max_pool_layer(random_tensor)
print(f"\n Max pool Tensor :\n {max_pool_tensor}")
print(f"Max pool tensor shape: {max_pool_tensor.shape}")

"""### 7.3 setup a loss function and optimizer for `model_2`"""

## Setup loss function
from helper_functions import accuracy_fn

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_2.parameters(),lr=0.1)

"""### 7.4 Trainning and Testing `model_2` using our trainning and testing functions."""

torch.manual_seed(42)
torch.cuda.manual_seed(42)

# measure time
from timeit import default_timer as timer
train_time_start_model_2 = timer()

# Train and test model
epochs =3
for epoch in tqdm(range(epochs)):
  print(f"Epoch: {epoch}\n-----")
  train_step(model=model_2,
             data_loader = train_dataloader,
             loss_fn=loss_fn,
             optimizer=optimizer,
             accuracy_fn=accuracy_fn,
             device=device)

  test_step(model=model_2,
            data_loader=test_dataloader,
            loss_fn=loss_fn,
            accuracy_fn=accuracy_fn,
            device=device)


train_time_end_model_2 = timer()

total_train_time_model_2 = print_train_time(start=train_time_start_model_2,
                                            end=train_time_end_model_2,
                                            device=device)

model_2_resuls = eval_model(model=model_2,
                            data_loader=test_dataloader,
                            loss_fn=loss_fn,
                            accuracy_fn=accuracy_fn,
                            device=device)

model_2_resuls

"""## 8. Compare model results and Trainning time."""

import pandas as pd
compare_results = pd.DataFrame([model_0_results,
                                model_1_results,
                                model_2_resuls]
                               )

compare_results

## Add trainning times to comparison
compare_results["trainning time"] = [total_train_time_model_0,
                                     total_train_time_model_1,
                                     total_train_time_model_2]

compare_results

## Visualize model results
compare_results.set_index('model_name')["model_acc"].plot(kind='barh')

"""## 9. Make and evaluate random predictions with best model."""

def make_predictions(model: torch.nn.Module,
                     data: list,
                     device: torch.device =device):
  pred_probs = []
  model.to(device)
  model.eval()

  with torch.inference_mode():
    for sample in data:

      sample = torch.unsqueeze(sample,dim=0).to(device)

      ## Forward pass
      pred_logit = model(sample)

      ## Get prediction probablility
      pred_prob = torch.softmax(pred_logit.squeeze(),dim=0)

      pred_probs.append(pred_prob.cpu())

## Stack the pred_prob to turn list into tensor

  return torch.stack(pred_probs)

img,label = test_data[0][:10]

img.shape,label

import random
#random.seed(42)
test_sample =[]
test_labels =[]

for sample,label in random.sample(list(test_data),k=9):
  test_sample.append(sample)
  test_labels.append(label)


test_sample[0].shape

plt.imshow(test_sample[0].squeeze(),cmap='gray')
plt.title(class_names[test_labels[0]])

## Make predictions

pred_probs = make_predictions(model=model_2,
                              data=test_sample)

## View first two prediction probababilities
pred_probs[:2]

## Convert prediction probabilities to labels
pred_classes = pred_probs.argmax(dim=1)
pred_classes

## Plot predictions
plt.figure(figsize=(9,9))
nrows=3
ncols=3

for i,sample in enumerate(test_sample):
  ## Create subpplot
  plt.subplot(nrows,ncols,i+1)

  plt.imshow(sample.squeeze(),cmap="gray")

  pred_label = class_names[pred_classes[i]]

  truth_label = class_names[test_labels[i]]

  title_text = f"Pred :{pred_label} | Truth :{truth_label}"


  if pred_label== truth_label:
    plt.title(title_text,fontsize=10,c="g")
  else:
    plt.title(title_text,fontsize=10,c="r")

  plt.axis(False)

